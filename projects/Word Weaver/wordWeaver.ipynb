{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "J1D42emD32Ro"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               phrases\n",
      "0                                                Hey ,\n",
      "1                                                 Hi ,\n",
      "2                                              Hello ,\n",
      "3                                       Good morning ,\n",
      "4                                     Good afternoon ,\n",
      "..                                                 ...\n",
      "563  With 10 years of experience in the mobile indu...\n",
      "564  Please let me know if you’re interested in col...\n",
      "565  She will be providing technical support and as...\n",
      "566  make sure they enjoy the best experience with ...\n",
      "567                                 Feel free to greet\n",
      "\n",
      "[568 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "file = pd.read_json(\"./prompts.json\")\n",
    "prompts = pd.DataFrame(file)\n",
    "print(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KhtDxwL_AXFj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'to': 1, 'you': 2, 'i': 3, 'the': 4, 'for': 5, 'your': 6, 'please': 7, 'this': 8, 'a': 9, 'and': 10, 'it': 11, 'in': 12, 'have': 13, 'me': 14, 'my': 15, 'with': 16, 'if': 17, 'would': 18, 'we': 19, 'on': 20, 'be': 21, 'of': 22, 'that': 23, 'our': 24, 'is': 25, 'thank': 26, 'any': 27, 'email': 28, 'as': 29, 'are': 30, 'about': 31, 'so': 32, 'am': 33, 'more': 34, 'know': 35, 'time': 36, 'could': 37, 'let': 38, 'us': 39, 'can': 40, 'like': 41, 'information': 42, 'hope': 43, 'at': 44, 'wanted': 45, 'some': 46, 'attached': 47, 'up': 48, 'help': 49, 'not': 50, 'get': 51, 'wednesday': 52, 'i’m': 53, 'great': 54, 'sorry': 55, 'just': 56, 'appreciate': 57, 'need': 58, 'free': 59, 'best': 60, 'from': 61, 'share': 62, 'was': 63, 'writing': 64, 'all': 65, 'here': 66, 'do': 67, 'will': 68, 'last': 69, 'i’d': 70, 'thanks': 71, 'had': 72, 'make': 73, 'what': 74, 'love': 75, 'keep': 76, 'inform': 77, 'may': 78, 'an': 79, 'out': 80, 'questions': 81, 'again': 82, 'look': 83, 'feedback': 84, 'work': 85, 'subscription': 86, 'forward': 87, 'inconvenience': 88, 'caused': 89, 'how': 90, 'take': 91, 'regret': 92, 'working': 93, 'but': 94, 'check': 95, 'possible': 96, 's': 97, 'been': 98, 'day': 99, 'documents': 100, 'feel': 101, 'understand': 102, 'touch': 103, 'call': 104, 'support': 105, 'has': 106, 'soon': 107, 'send': 108, 'new': 109, 'find': 110, 'offer': 111, 'set': 112, 'discuss': 113, 'kindly': 114, 'document': 115, 'it’s': 116, 'afraid': 117, 'much': 118, 'file': 119, 'learn': 120, 'good': 121, 'week': 122, 'provide': 123, 'request': 124, 'reach': 125, 'apologize': 126, 'assure': 127, 'through': 128, 'experience': 129, 'previous': 130, 'back': 131, 'confidential': 132, 'sharing': 133, 'now': 134, 'give': 135, 'per': 136, 'unfortunately': 137, 'hear': 138, 'next': 139, 'schedule': 140, 'i’ve': 141, 'really': 142, 'i’ll': 143, 'there': 144, 'you’re': 145, 'connect': 146, 'details': 147, 'concerns': 148, 'requested': 149, 'further': 150, 'future': 151, 'apology': 152, 'deeply': 153, 'want': 154, 'over': 155, 'shared': 156, 'following': 157, 'review': 158, 'contract': 159, 'vendor': 160, 'appreciated': 161, 'or': 162, 'don’t': 163, 'hesitate': 164, 'accept': 165, 'tell': 166, 'invite': 167, 'shortly': 168, 'summer': 169, 'products': 170, 'excited': 171, 'team': 172, 'start': 173, 'helpful': 174, 'considering': 175, 'address': 176, 'than': 177, 'happy': 178, 'end': 179, 'apologies': 180, 'reply': 181, 'right': 182, 'go': 183, 'making': 184, 'follow': 185, 'got': 186, 'meeting': 187, 'let’s': 188, 'project': 189, 'quick': 190, 'discussed': 191, 'anything': 192, 'continued': 193, 'insights': 194, 'news': 195, 'proceed': 196, 'cancellation': 197, 'company': 198, 'see': 199, 'very': 200, 'additional': 201, 'by': 202, 'highly': 203, 'when': 204, 'looking': 205, 'contact': 206, 'linkedin': 207, 'catalog': 208, 'read': 209, 'opportunities': 210, 'explore': 211, 'today': 212, 'leave': 213, 'discount': 214, 'well': 215, 'having': 216, 'meet': 217, 'event': 218, 'specific': 219, 'potential': 220, 'reaching': 221, 'taking': 222, 'write': 223, 'regarding': 224, 'pfa': 225, 'answer': 226, 'late': 227, 'issue': 228, 'product': 229, 'matter': 230, 'token': 231, 'reminder': 232, 'keeping': 233, 'quickly': 234, 'did': 235, 'treat': 236, 'ensure': 237, 'such': 238, 'mind': 239, 'clear': 240, 'expectations': 241, 'highlight': 242, 'far': 243, 'convenience': 244, 'collaboration': 245, 'valuable': 246, 'without': 247, 'pleasure': 248, 'name': 249, 'regards': 250, 'talk': 251, 'understanding': 252, 'greatly': 253, 'assistance': 254, 'these': 255, 'ask': 256, 'took': 257, 'started': 258, 'enjoyed': 259, 'why': 260, 'everyone': 261, 'explain': 262, 'said': 263, 'wish': 264, 'available': 265, 'able': 266, 'unable': 267, 'after': 268, 'due': 269, 'thursday': 270, 'tomorrow': 271, 'recently': 272, 'you’d': 273, 'coming': 274, 'friday': 275, 'also': 276, 'before': 277, 'add': 278, 'recommended': 279, 'purchase': 280, 'plan': 281, 'personalized': 282, 'stay': 283, 'role': 284, 'miss': 285, 'sure': 286, 'hey': 287, 'hi': 288, 'hello': 289, 'morning': 290, 'dear': 291, 'concern': 292, 'amazing': 293, 'weekend': 294, 'glad': 295, 'solutions': 296, 'case': 297, 'other': 298, 'steps': 299, 'repeat': 300, 'extend': 301, 'full': 302, 'actively': 303, 'raised': 304, 'negative': 305, 'committed': 306, 'service': 307, 'situation': 308, 'solution': 309, 'friendly': 310, 'conversation': 311, 'must': 312, 'chance': 313, 'into': 314, 'remind': 315, 'task': 316, 'updates': 317, 'brainstorm': 318, 'believe': 319, 'reference': 320, 'sensitive': 321, 'remains': 322, 'else': 323, 'heads': 324, 'attaching': 325, 'both': 326, 'refer': 327, 'discussion': 328, 'records': 329, 'gratitude': 330, 'enthusiasm': 331, 'prompt': 332, 'hard': 333, 'bad': 334, 'way': 335, 'canceling': 336, 'requesting': 337, 'termination': 338, 'serves': 339, 'official': 340, 'notice': 341, 'cancel': 342, 'communication': 343, 'chat': 344, 'sincerely': 345, 'wishes': 346, 'during': 347, 'hearing': 348, 'consideration': 349, 'interest': 350, 'challenges': 351, 'we’ll': 352, 'wonderful': 353, 'bottom': 354, 'attending': 355, 'together': 356, 'thoughts': 357, 'sending': 358, 'replying': 359, 'responding': 360, 'response': 361, 'changes': 362, 'sign': 363, 'grateful': 364, 'didn’t': 365, 'fully': 366, 'quite': 367, 'light': 368, 'use': 369, 'they': 370, 'already': 371, 'loop': 372, 'think': 373, 'views': 374, 'waiting': 375, 'via': 376, 'comfortable': 377, 'feasible': 378, 'book': 379, 'accordingly': 380, 'won’t': 381, 'scheduled': 382, 'taken': 383, 'against': 384, 'rules': 385, 'efforts': 386, 'domain': 387, 'advance': 388, 'everything': 389, 'ready': 390, 'meantime': 391, 'patience': 392, 'luck': 393, 'welcome': 394, 'leaving': 395, 'open': 396, 'came': 397, 'post': 398, 'options': 399, 'later': 400, 'we’ve': 401, 'walk': 402, 'packages': 403, '12': 404, '–': 405, '5': 406, 'p': 407, 'm': 408, 'est': 409, 'chatting': 410, 'save': 411, 'improve': 412, 'suggestions': 413, 'ps': 414, 'lot': 415, 'say': 416, 'skills': 417, 'video': 418, 'current': 419, 'portfolio': 420, 'creative': 421, 'opportunity': 422, 'past': 423, 'learning': 424, 'career': 425, 'coffee': 426, 'subscribe': 427, 'network': 428, 'repost': 429, 'pin': 430, 'survey': 431, 'exclusive': 432, 'tailored': 433, 'tickets': 434, 'discover': 435, 'customer': 436, 'cta': 437, 'examples': 438, 'rate': 439, 'customized': 440, 'special': 441, 'connected': 442, 'reached': 443, 'job': 444, 'done': 445, 'years': 446, 'going': 447, 'sick': 448, 'absence': 449, '10': 450, 'store': 451, 'student': 452, 'interested': 453, 'afternoon': 454, 'evening': 455, 'sir': 456, 'madam': 457, 'whom': 458, 'whomsoever': 459, 'finds': 460, 'restful': 461, 'inquire': 462, 'introduce': 463, 'immensely': 464, 'issues': 465, 'causing': 466, 'delay': 467, \"you're\": 468, 'wondering': 469, 'question': 470, 'misunderstanding': 471, 'necessary': 472, 'resolve': 473, 'heartfelt': 474, 'responsibility': 475, 'happened': 476, 'shall': 477, 'happen': 478, 'trouble': 479, 'you’ve': 480, 'delays': 481, 'resolving': 482, 'rest': 483, 'assured': 484, 'immediate': 485, 'action': 486, 'rectify': 487, 'prevent': 488, 'recurrence': 489, 'compensation': 490, 'offering': 491, 'alternative': 492, 'base': 493, 'busy': 494, 'deadline': 495, 'close': 496, 'circle': 497, 'aside': 498, 'arrange': 499, 'block': 500, 'calendar': 501, 'half': 502, 'hour': 503, \"3'o\": 504, 'clock': 505, 'availability': 506, 'only': 507, 'between': 508, 'anyone': 509, 'strictly': 510, 'outline': 511, 'same': 512, 'page': 513, 'deadlines': 514, 'avoid': 515, 'misunderstandings': 516, 'rundown': 517, 'attachment': 518, 'earliest': 519, 'required': 520, 'perusal': 521, 'moment': 522, 'wouldn’t': 523, 'difficult': 524, 'deliver': 525, 'effective': 526, 'cheers': 527, 'one': 528, 'until': 529, 'yours': 530, 'warmly': 531, 'catching': 532, 'attention': 533, 'proves': 534, 'useful': 535, 'once': 536, 'cooperation': 537, 'partnership': 538, 'update': 539, 'promptly': 540, 'clarification': 541, 'trip': 542, 'away': 543, 'needed': 544, 'holiday': 545, 'season': 546, 'convention': 547, 'purpose': 548, 'yesterday’s': 549, 'presentation': 550, 'informative': 551, 'concerning': 552, 'connection': 553, 'interaction': 554, 'received': 555, 'went': 556, 'yesterday': 557, 'month': 558, 'suggestion': 559, 'asking': 560, 'longer': 561, 'usual': 562, 'while': 563, 'since': 564, 'condolences': 565, 'inconveniences': 566, 'attachments': 567, 'enclosed': 568, 'made': 569, 'bold': 570, 'red': 571, 'blue': 572, 'comments': 573, 'sent': 574, 'pdf': 575, 'contents': 576, 'form': 577, 'here’s': 578, 'two': 579, 'main': 580, 'below': 581, 'clarifications': 582, 'shed': 583, 'topic': 584, 'clarify': 585, 'feature': 586, 'mistaken': 587, 'mean': 588, 'words': 589, 'phrases': 590, 'someone': 591, 'told': 592, 'notifying': 593, 'note': 594, 'informed': 595, 'posted': 596, 'updated': 597, 'approval': 598, 'ok': 599, 'thumbs': 600, 'green': 601, 'totally': 602, 'ahead': 603, 'he': 604, 'approved': 605, 'monday': 606, 'convenient': 607, 'interview': 608, 'rescheduled': 609, 'cannot': 610, 'careful': 611, 'decision': 612, 'rain': 613, 'seems': 614, 'unlikely': 615, 'put': 616, 'despite': 617, 'putting': 618, 'answered': 619, 'there’s': 620, 'okay': 621, 'confident': 622, 'problem': 623, 'trust': 624, 'inquiry': 625, 'accepting': 626, 'applications': 627, 'graduate': 628, 'program': 629, 'endeavors': 630, 'refrain': 631, 'window': 632, 'desk': 633, 'each': 634, 'night': 635, 'water': 636, 'cubicle': 637, 'saw': 638, 'own': 639, 'operate': 640, 'twenty': 641, 'minutes': 642, 'carry': 643, 'pricing': 644, 'few': 645, 'slots': 646, 'which': 647, 'crucial': 648, 'aspect': 649, 'however': 650, 'consuming': 651, 'challenging': 652, 'where': 653, 'easy': 654, 'hours': 655, 'otherwise': 656, 'instead': 657, 'addition': 658, 'quality': 659, 'analyzing': 660, 'generating': 661, 'improvement': 662, 'recommend': 663, 'trying': 664, 'signing': 665, 'trial': 666, 'began': 667, 'article': 668, 'strong': 669, 'opinions': 670, 'conducted': 671, 'research': 672, 'realized': 673, 'most': 674, 'completing': 675, 'intensive': 676, 'confidently': 677, 'changed': 678, 'fact': 679, 'piece': 680, 'maintain': 681, 'ways': 682, 'discovered': 683, 'freelance': 684, 'writer': 685, 'restaurant': 686, 'hospitality': 687, 'brands': 688, 'include': 689, 'lose': 690, 'track': 691, 'list': 692, 'discovery': 693, 'found': 694, 'profile': 695, 'seeing': 696, 'opening': 697, 'latest': 698, 'across': 699, 'researching': 700, 'particularly': 701, 'fit': 702, 'company’s': 703, 'needs': 704, 'background': 705, 'contribute': 706, 'success': 707, 'brief': 708, 'connecting': 709, 'seen': 710, 'grow': 711, 'looks': 712, 'thriving': 713, 'currently': 714, 'passion': 715, 'peers': 716, 'strengthened': 717, 'skill': 718, 'mutually': 719, 'beneficial': 720, 'course': 721, 'challenge': 722, 'experiences': 723, 'short': 724, 'virtual': 725, 'weeks': 726, 'possibility': 727, 'buy': 728, 'appointment': 729, 'guide': 730, 'features': 731, 'tiktok': 732, 'awaits': 733, 'order': 734, 'cart': 735, 'claim': 736, 'join': 737, 'community': 738, 'upgrade': 739, 'services': 740, 'blog': 741, 'works': 742, 'watch': 743, 'testimonials': 744, 'faqs': 745, 'view': 746, 'resources': 747, 'social': 748, 'friends': 749, 'spread': 750, 'word': 751, 'followers': 752, 'facebook': 753, 'tag': 754, 'friend': 755, 'snap': 756, 'opinion': 757, 'voice': 758, 'invitation': 759, 'deals': 760, 'unlock': 761, 'ideal': 762, 'quote': 763, 'lovely': 764, 'transition': 765, 'learned': 766, 'aren’t': 767, 'huge': 768, 'growth': 769, 'exciting': 770, 'colleagues': 771, 'afforded': 772, 'proud': 773, 'year': 774, 'although': 775, 'chapter': 776, 'goodbye': 777, 'person': 778, 'sad': 779, 'excellent': 780, 'coworkers': 781, 'joy': 782, 'part': 783, 'spending': 784, 'freedom': 785, 'chats': 786, 'cheering': 787, 'retirement': 788, 'internship': 789, 'helped': 790, 'couldn’t': 791, 'bittersweet': 792, 'embark': 793, 'mentorship': 794, 'invaluable': 795, 'emailing': 796, 'account': 797, 'come': 798, 'fever': 799, 'checking': 800, 'emails': 801, 'urgent': 802, 'cell': 803, 'phone': 804, 'aim': 805, 'illness': 806, 'being': 807, 'formally': 808, 'period': 809, 'medical': 810, 'condition': 811, 'arisen': 812, 'doctor': 813, 'extended': 814, 'off': 815, 'recover': 816, 'expected': 817, 'smooth': 818, 'workflow': 819, 'refunded': 820, 'funds': 821, 'should': 822, 'promo': 823, 'code': 824, 'unpleasant': 825, 'frustration': 826, 'forwarded': 827, 'complaint': 828, 'management': 829, 'never': 830, 'happens': 831, 'discounts': 832, 'annual': 833, 'coding': 834, 'conference': 835, 'university': 836, 'ticket': 837, 'price': 838, 'too': 839, 'high': 840, 'educational': 841, 'complete': 842, 'projects': 843, 'pending': 844, 'tasks': 845, 'vacation': 846, 'website': 847, 'still': 848, 'guest': 849, 'mobile': 850, 'industry': 851, 'audience': 852, 'collaborating': 853, 'she': 854, 'providing': 855, 'technical': 856, 'users': 857, 'enjoy': 858, 'greet': 859}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# assigns each word a unique number/index\n",
    "tokenizer.fit_on_texts(prompts.phrases)\n",
    "\n",
    "print(tokenizer.word_index)\n",
    "# for s in tokenizer.word_index:\n",
    "    # print(s for s )\n",
    "# help(tokenizer)\n",
    "\n",
    "# print(tokenizer.word_index['Hey'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "44VahqKdAjr9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   0   0 287]\n",
      " [  0   0   0 ...   0   0 288]\n",
      " [  0   0   0 ...   0   0 289]\n",
      " ...\n",
      " [  0   0   0 ...   0 101  59]\n",
      " [  0   0   0 ... 101  59   1]\n",
      " [  0   0   0 ...  59   1 859]]\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_padded_sequences(phrases):\n",
    "    \n",
    "  input_sequences = []\n",
    "  # loop for each given sentence in list of sentences phrases\n",
    "  for phrase in phrases:\n",
    "    \n",
    "    # texts_to_sequences convert the sentence to list of indexes corresponding to word in dictionary\n",
    "    tokenized_sentence = tokenizer.texts_to_sequences([phrase])[0]\n",
    "    # print(tokenized_sentence)\n",
    "    # training for each sentence by iterating over no. of words taken to train in an iteration\n",
    "    for i in range(len(tokenized_sentence)):\n",
    "      input_sequences.append(tokenized_sentence[:i+1])\n",
    "  global vectorLength \n",
    "  vectorLength = max([len(x) for x in input_sequences])\n",
    "  \n",
    "  # empty list of sequences initialized\n",
    "  padded_input_sequences = []\n",
    "  \n",
    "  # padded_input_sequences put zeroes in place of empty words\n",
    "  # e.g. my sentence is                      -   \"I hope this email finds you well\"\n",
    "  # after mapping to index in dectionary     -   \"3  42   7    28    444   2  212\"  \n",
    "  # padding of zeros are added in beginnning -   \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 42 7 28 444 2 212\" \n",
    "  # now the length of vector becomes 25 with zeros added\n",
    "  padded_input_sequences = pad_sequences(input_sequences, maxlen = vectorLength, padding='pre')\n",
    "  return padded_input_sequences\n",
    "\n",
    "X = sentence_to_padded_sequences(prompts.phrases)\n",
    "print(X)\n",
    "# print(X[24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "rs1NPitwSgzk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[287 288 289 ...  59   1 859]\n"
     ]
    }
   ],
   "source": [
    "X = sentence_to_padded_sequences(prompts.phrases)[:,:-1]\n",
    "y = sentence_to_padded_sequences(prompts.phrases)[:,-1]\n",
    "print(y)\n",
    "dictionaryLength = len(tokenizer.word_index)\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# convert the desired values to an array where all values are zero except at dictionary index which is one\n",
    "y = to_categorical(y,num_classes=dictionaryLength+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not interpret initializer identifier: 24",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# tf.keras.models\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# class weaveLSTM(tf.keras.Model):\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# def __init__(self):\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m#     x = self.Dense(x)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m#     return x\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m---> 17\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mdictionaryLength\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorLength\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m(X)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# x = keras.layers.Dense(vectorLength-1)(X)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m x2 \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mLSTM(dictionaryLength\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)(x1)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\dtensor\\utils.py:96\u001b[0m, in \u001b[0;36mallow_initializer_layout.<locals>._wrap_function\u001b[1;34m(layer_instance, *args, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m layout:\n\u001b[0;32m     94\u001b[0m             layout_args[variable_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_layout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m layout\n\u001b[1;32m---> 96\u001b[0m \u001b[43minit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Inject the layout parameter after the invocation of __init__()\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layout_param_name, layout \u001b[38;5;129;01min\u001b[39;00m layout_args\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\embedding.py:159\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[1;34m(self, input_dim, output_dim, embeddings_initializer, embeddings_regularizer, activity_regularizer, embeddings_constraint, mask_zero, input_length, sparse, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim \u001b[38;5;241m=\u001b[39m input_dim\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim \u001b[38;5;241m=\u001b[39m output_dim\n\u001b[1;32m--> 159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings_initializer \u001b[38;5;241m=\u001b[39m \u001b[43minitializers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings_initializer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings_regularizer \u001b[38;5;241m=\u001b[39m regularizers\u001b[38;5;241m.\u001b[39mget(embeddings_regularizer)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivity_regularizer \u001b[38;5;241m=\u001b[39m regularizers\u001b[38;5;241m.\u001b[39mget(activity_regularizer)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\initializers\\__init__.py:223\u001b[0m, in \u001b[0;36mget\u001b[1;34m(identifier)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m identifier\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 223\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not interpret initializer identifier: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(identifier)\n\u001b[0;32m    225\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Could not interpret initializer identifier: 24"
     ]
    }
   ],
   "source": [
    "# tf.keras.models\n",
    "# class weaveLSTM(tf.keras.Model):\n",
    "    # def __init__(self):\n",
    "        # super().__init__(self)\n",
    "    #     self.inputLayer = tf.keras.Input(shape = (vectorLength-1, 1))\n",
    "    #     self.LSTM = LSTM(dictionaryLength+1)\n",
    "    #     self.Dense = Dense(dictionaryLength+1, activation=\"sigmoid\")\n",
    "    \n",
    "    # def call(self, input):\n",
    "    #     x = input\n",
    "    #     x = self.inputLayer(x)\n",
    "    #     x = self.LSTM(x)\n",
    "    #     x = self.Dense(x)\n",
    "    #     return x\n",
    "\n",
    "from tensorflow import keras\n",
    "x1 = keras.layers.Embedding( dictionaryLength+1, vectorLength-1)(X)\n",
    "# x = keras.layers.Dense(vectorLength-1)(X)\n",
    "x2 = keras.layers.LSTM(dictionaryLength+1)(x1)\n",
    "x3 = keras.layers.Dense(dictionaryLength+1, activation=\"sigmoid\")(x2)\n",
    "\n",
    "model = keras.Model(inputs=X, outputs=x3)\n",
    "keras.utils.plot_model(model, \"my_first_model.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# creating our LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(dictionaryLength+1, 400, input_length=vectorLength-1))\n",
    "model.add(LSTM(dictionaryLength+1))\n",
    "model.add(Dense( dictionaryLength+1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "# model.summary()\n",
    "keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LpFUCALCfJRR",
    "outputId": "96d67a78-3c2e-4462-b2a8-2655c303af8d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\Sachit\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Sachit\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "151/151 [==============================] - 79s 430ms/step - loss: 5.7564 - accuracy: 0.0662\n",
      "Epoch 2/10\n",
      "151/151 [==============================] - 62s 412ms/step - loss: 4.9854 - accuracy: 0.1300\n",
      "Epoch 3/10\n",
      "151/151 [==============================] - 56s 369ms/step - loss: 4.1982 - accuracy: 0.2082\n",
      "Epoch 4/10\n",
      "151/151 [==============================] - 525s 4s/step - loss: 3.3854 - accuracy: 0.2996\n",
      "Epoch 5/10\n",
      "137/151 [==========================>...] - ETA: 4s - loss: 2.5693 - accuracy: 0.4131"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X,y,epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PGeYGwCMfTus",
    "outputId": "2d508555-b83e-470e-e7e5-1b5c10cce70b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "0.99995625\n",
      "could you please\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "0.9998741\n",
      "could you please let\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "0.9999998\n",
      "could you please let us\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "0.9999842\n",
      "could you please let us know\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "0.9999687\n",
      "could you please let us know what\n"
     ]
    }
   ],
   "source": [
    "def predict(sentence: str):\n",
    "    \"\"\"\n",
    "    Prints the word predictions based on a given sentence, limited to a maximum of N words\n",
    "    ## Parameters\n",
    "    sentence : string which is the initial sentence \n",
    "    \"\"\"\n",
    "    for i in range(5):\n",
    "        \n",
    "        # tokenize\n",
    "        token_text = tokenizer.texts_to_sequences([sentence])[0]\n",
    "        \n",
    "        # padding\n",
    "        padded_token_text = pad_sequences([token_text], maxlen=vectorLength-1, padding='pre')\n",
    "        \n",
    "        # predict\n",
    "        arr = model.predict(padded_token_text)\n",
    "        print(max(arr[0]))\n",
    "\n",
    "        # if it is predicting 6th word and if it's probability is lower than 0.2 it should not predict word as it will be absurd predictions\n",
    "        if(max(arr[0]) < 0.20 and i>4):\n",
    "            break\n",
    "        pos = np.argmax(arr)\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == pos:\n",
    "                sentence = sentence + \" \" + word\n",
    "                print(sentence)\n",
    "\n",
    "\n",
    "text = \"could you\"\n",
    "predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# saving the model and the dictionary in the extension folder\n",
    "# import joblib\n",
    "# joblib.dump(model, \"./extension/model\")\n",
    "# model.save(\"./extension/model.h5\")\n",
    "# tokenizer_json = tokenizer.to_json()\n",
    "# with open('./extension/tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "#     f.write(json.dumps(tokenizer_json, ensure_ascii=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"model\")\n",
    "# import tensorflowjs\n",
    "# tensorflowjs.converters.save_keras_model(model, \"extension\")\n",
    "# model_json = model.to_json()\n",
    "# with open(\"./extension/model.json\", \"w\") as f:\n",
    "#     json.dump(model_json, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
